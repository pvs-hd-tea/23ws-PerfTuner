WEBVTT

00:00.000 --> 00:16.640
Hello everyone, we are the perf tuner team and our project revolves around we have a

00:16.640 --> 00:23.920
C++ function or a program and we have to convert it or optimize it using AVX and of course

00:23.920 --> 00:31.720
via LLMs. So the progress for this week is that we have an optimized or a better version

00:31.720 --> 00:39.120
of our perf tuner and we have thought about our tree of thoughts. Also we have worked

00:39.120 --> 00:48.360
on the Gaussian elimination example by hand. This was the flow chart that we already presented

00:49.360 --> 00:55.080
and we have implemented it and we have code ready for it. So it is working but there are

00:55.080 --> 01:01.080
still some errors and issues that I still have to rectify and this is the initial prompt

01:01.080 --> 01:08.200
that I am putting into the chat GPT API and as we already see there are a lot of specifics

01:08.200 --> 01:13.760
that I really had to mention so that it produced proper results and further that we are looking

01:13.840 --> 01:17.760
into examples we see that there are a lot of things that we really have to mention for

01:17.760 --> 01:24.800
chat GPT to understand the kind of results that we want. This is where I am sort of stuck

01:24.800 --> 01:34.120
right now. So as of now it is compiling. The results are compiling in the Python environment

01:34.120 --> 01:39.240
which is the C++ program and the program which is vectorized. They are all compiling in the

01:39.240 --> 01:46.640
Python environment. However one problem is that we are getting a higher execution time for AVX

01:46.640 --> 01:53.040
program in comparison to C++ program and the reason is because in the program we are converting

01:53.040 --> 01:59.080
the input data into vectorized form and then doing the processing. So this is taking more time. This

01:59.080 --> 02:05.200
is the reason time is more. So I need to figure out a way to actually find out how to find the

02:05.200 --> 02:07.600
execution time without this conversion.

02:07.600 --> 02:37.440
So that is one thing and the other one is that I am getting the syntax of the command

02:37.440 --> 02:43.800
is incorrect. This kind of output which is from the ID not from the script. So I am still figuring

02:43.800 --> 02:49.560
out why this is not working because in a G++ environment it is perfectly compiling and giving

02:49.560 --> 02:58.680
the results. So this is yet to be found out what is happening here. Yeah and then I like thought

02:58.680 --> 03:03.920
a bit more in general because what we tried so far what we had the program and we tried to tell

03:04.440 --> 03:12.000
GPT like optimize it using AVX in different ways but that didn't always work perfectly so I

03:12.000 --> 03:18.760
tried to think of it bigger and constructed like a tree of thoughts where this is only a subtask

03:18.760 --> 03:26.520
and where we have like results that are also good even if this subtask doesn't work and then I first

03:26.520 --> 03:32.520
like all this is done by hand first and next week I try to make it like into a program and

03:32.520 --> 03:39.800
the first task was if we have a C++ file I asked which subtasks are tackled to see if the program

03:39.800 --> 03:45.800
actually can find what we are doing and this worked pretty fine. Actually it could structure

03:45.800 --> 03:51.680
what we are doing in every step but there was a minor problem that it also said something like

03:51.680 --> 03:58.160
C out or declaring a variable is actually a subtask but then we could ask in another

03:58.160 --> 04:02.840
step there actually which subtask can we improve and then he found out that like declaring a

04:02.840 --> 04:09.320
variable is not to improvement because it's just a language so he could actually find in like a

04:09.320 --> 04:14.000
long file which are the problems that we can actually improve which I think is already pretty

04:14.000 --> 04:20.360
interesting and then we could ask him how he would improve them and then he found different ways

04:20.360 --> 04:25.600
telling for example numerical stability or storage because I just asked like very widely

04:25.600 --> 04:32.800
but also parallelization he found and then we could ask him like how he would do parallelization

04:32.800 --> 04:39.800
and then he said either multi-threaded programming or using AVX and this is like the first half of

04:39.800 --> 04:44.840
the tree which is just about understanding what is actually the input which are the tasks which

04:44.840 --> 04:53.000
we can tackle and how we could improve them and the red is then what like leads to that end what

04:53.000 --> 05:00.680
we want yeah well we wouldn't continue a green is where we can actually continue and orange is

05:00.680 --> 05:06.440
like potential to continue like if there would be like in I don't know five years if PerfQ

05:06.440 --> 05:12.200
is still existing and someone decides to improve storage then he could actually get in there but

05:12.240 --> 05:18.280
we will continue with the parallelization path and then the other question was how we can improve

05:18.280 --> 05:28.280
it and so far we were always in this like left line that basically trying to tell how to make

05:28.280 --> 05:32.920
better code this is like just an example and then also with the dots you see like still not

05:32.920 --> 05:42.080
perfectly but then what we also found out when Alexander was doing the Gauss elimination that

05:42.080 --> 05:49.840
sometimes just seems impossible telling chat GPT how to actually use AVX in some cases and in the

05:49.840 --> 05:55.160
end we were basically telling him what we could write ourselves so that won't be any help but then

05:55.160 --> 06:00.400
we thought what we actually still have we have like a solution done by hand and very often in

06:00.400 --> 06:07.200
scientific programs there are tasks that occur all the time like transposing a matrix or doing

06:07.200 --> 06:15.200
like a matrix vector product or doing a dot product and if we have like a database of 20 years then

06:15.200 --> 06:21.160
that sub task and we already know them how to do them by hand and we teach chat GPT like chat GPT

06:21.160 --> 06:26.640
gets the sub task we teach them which is the best solution just say like substitute it and if we

06:26.640 --> 06:31.960
have like a long file where he does like 20 times dot product and we tell him to substitute it that's

06:31.960 --> 06:39.120
already a big help so this could be like the one path that works if we have like a potential for

06:39.120 --> 06:45.720
substitution the other case would of course still be to try that chat GPT actually optimizes it

06:45.720 --> 06:51.880
himself but could also be that that not worked so in like if we check it and we came to the case

06:51.880 --> 06:57.520
that both doesn't work we still have something that we found out that is the parallelization

06:57.520 --> 07:05.320
potential because we divided the tasks and found out how we can improve them we found tasks where

07:05.320 --> 07:10.840
we can parallelize and where we can use AVX and then if we have like in the worst case a file with

07:10.840 --> 07:16.520
300 lines but we tell to the user in this three lines we think you could use AVX even if you don't

07:16.520 --> 07:23.000
give the solution he knows where to look which is already a good but if we then actually find a

07:23.000 --> 07:30.000
solution we for like every sub task we could try to merge them and give like a better C++ file plus

07:30.000 --> 07:38.240
the information for the tasks where it didn't work so we have like three trials to use chat GPT to

07:38.240 --> 07:45.640
make better code and hope that like at least some of them will work for some sub tasks and then now

07:45.640 --> 07:52.000
where we are like which is still the biggest intellectual problem where we still need to think

07:52.000 --> 07:59.760
and it's not just about like implementing it is this line where we try to make chat GPT to better

07:59.760 --> 08:05.120
code without having knowledge about the problem and there we like still trying a bit around what

08:05.120 --> 08:11.360
works because we want to have like a line of prompting that works for like any or in theory

08:11.360 --> 08:18.920
for any problem and this is where we still need to think a bit. That's where I come in I looked at

08:18.920 --> 08:25.960
the example from last week from Peter Bastian maybe remember it and I looked at this example

08:25.960 --> 08:34.080
more deeply to find the obstacles that chat GPT has to give us a correct code. As a brief recap

08:34.080 --> 08:39.440
we have the Gaussian elimination system so you have a system of linear equations and by

08:39.440 --> 08:47.280
like step by step eliminating each variable you get the solution for this system of linear

08:47.280 --> 08:52.440
equations and you can do this just as you've learned in high school step by step however you

08:52.440 --> 09:00.120
can also do this by changing the order of what you add like which rows you add on top of which

09:00.120 --> 09:08.440
one and you do this for casual use so you can do it quicker and so I made a sketch of the entire

09:08.440 --> 09:14.880
matrix and you divide it up into certain blocks denoted by the black squares and then how you

09:14.880 --> 09:21.480
can do it you can first start with the upper left block in the left corner and then you do the next

09:21.480 --> 09:26.880
row and then the column. The reason why you do this this way is because you need the result of

09:26.920 --> 09:34.960
the first block to be able to do the rest of the row and the block and then you can do the rest of

09:34.960 --> 09:42.920
the matrix. So that's behind it let's first look at the upper left block so this code is only

09:42.920 --> 09:49.800
responsible for this upper left block here and then you can see the unrolling of the loop you

09:49.800 --> 09:56.560
in the very upper loop you have a step size of B which is the size of the block and then you have

09:56.560 --> 10:02.560
some other loops and when you come to this line here you can do the actual arithmetics and you

10:02.560 --> 10:08.520
can use AVX functions very nicely however there's a problem if you look at this loop actually the

10:08.520 --> 10:14.760
times this loop runs changes with K and then actually in the end it might happen that this

10:14.760 --> 10:22.160
loop only runs for three times or two times and then you load your elements into the register and

10:22.160 --> 10:27.280
load it back then you actually overwrite other elements of the matrix they actually overwrite

10:27.280 --> 10:33.280
elements here and this obviously gives you wrong results and Chachapiti is not aware of that at

10:33.280 --> 10:42.840
all. I looked at I wanted to optimize it by hand and I'm normally looking at the red square which

10:42.840 --> 10:49.520
is the bulk of it and here you actually can do a really nice optimization but also you have to be

10:49.520 --> 10:55.480
really careful this is the last so this is the command you want to substitute however there are

10:55.480 --> 11:00.680
two problems if you only look at this loop and you want to kind of change the step size here to

11:00.680 --> 11:07.480
four to use the AVX intrinsic but there are two problems the first problem is this element stays

11:07.480 --> 11:14.880
the same and you add onto this element and there are no such commands intrinsic commands to do that

11:14.880 --> 11:22.280
and the second problem is here you change K here works nicely but here you have to change the K

11:22.280 --> 11:30.300
but since the way this matrix is stored if I load it only get I can tell them okay load this element

11:30.300 --> 11:38.640
and the next three ones in memory but the next three in memory are not the next K one so K plus

11:38.640 --> 11:44.880
one K plus two but J plus one J plus two so you look wrote the wrong elements and also Chachapiti

11:44.880 --> 11:51.400
is not capable of doing that if you want to do it correctly you actually can see well the loop above

11:51.400 --> 11:58.080
it it's possible to change the step size to four then it works nicely then it's no problem and then

11:58.080 --> 12:04.720
you can do it and this runs fast and gives the correct results but this I've done by hand then

12:04.720 --> 12:13.600
I tried Chachapiti to give me results and I really had to give him very very specific commands which

12:13.600 --> 12:19.440
are very specific to this problem and obviously we want a more general approach so that we can

12:19.440 --> 12:25.800
not only use this example which we already have and now we are looking maybe at this example another

12:25.800 --> 12:32.080
example this is maybe also a question this is connecting to the last question from Thomas that

12:32.080 --> 12:38.280
like we really need to figure out what are the obstacles and how can we in a general form tell

12:38.280 --> 12:45.040
Chachapiti to tackle these obstacles and yeah it's actually I understand why Chachapiti took so long

12:45.040 --> 12:50.640
it also took me quite a while to understand the obstacle what's what's going on here yeah I could

12:50.640 --> 12:58.800
it worked when I told him I can change the step size only for the rest matrix only in the J loop

12:58.920 --> 13:04.280
and then even gave me a wrong result because he actually moved this command outside of K which

13:04.280 --> 13:09.440
couldn't even compile afterwards because K is not known outside the loop I had to tell him move it in

13:09.440 --> 13:16.080
the loop and then I got the correct result which is like way too specific but I think if we now look at

13:16.080 --> 13:23.360
another example maybe the transpose example we might get a feeling what are in general approach

13:23.360 --> 13:32.440
to tackle these obstacles. Let me stop the recording

